{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import pathlib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataset_structure(base_dir):\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {base_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìÇ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {base_dir}\")\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        split_path = os.path.join(base_dir, split)\n",
    "        if not os.path.exists(split_path):\n",
    "            print(f\"  ‚õî ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {split}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüîπ {split.upper()} SET:\")\n",
    "        for class_name in sorted(os.listdir(split_path)):\n",
    "            class_path = os.path.join(split_path, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                image_count = len([\n",
    "                    f for f in os.listdir(class_path)\n",
    "                    if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "                ])\n",
    "                print(f\"  üìÅ {class_name:<20} ‚Äî {image_count:>3}\")\n",
    "        print(\"\")\n",
    "\n",
    "inspect_dataset_structure(\"resized_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with your original settings\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"resized_images/train\",\n",
    "    image_size=(200, 200),\n",
    "    batch_size=32,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"resized_images/val\",\n",
    "    image_size=(200, 200),\n",
    "    batch_size=32,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "num_classes = len(train_ds.class_names)\n",
    "print(f\"Classes: {train_ds.class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data augmentation for multi-leaf scenes\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.3),                    # Higher for field conditions\n",
    "    layers.RandomZoom(0.3),                        # Increased for multi-leaf\n",
    "    layers.RandomContrast(0.4),                    # Handle lighting variations\n",
    "    layers.RandomBrightness(0.3),                  # Handle shadows/sunlight\n",
    "    layers.RandomTranslation(0.2, 0.2),            # Handle leaf positioning\n",
    "    layers.CenterCrop(200, 200),\n",
    "])\n",
    "\n",
    "# Multi-leaf attention mechanisms\n",
    "def spatial_attention_block(x):\n",
    "    avg_pool = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "    max_pool = tf.reduce_max(x, axis=-1, keepdims=True)\n",
    "    concat = layers.Concatenate(axis=-1)([avg_pool, max_pool])\n",
    "    attention = layers.Conv2D(1, 7, padding='same', activation='sigmoid')(concat)\n",
    "    return layers.Multiply()([x, attention])\n",
    "\n",
    "def channel_attention_block(x, filters):\n",
    "    avg_pool = layers.GlobalAveragePooling2D()(x)\n",
    "    max_pool = layers.GlobalMaxPooling2D()(x)\n",
    "    avg_pool = layers.Reshape((1, 1, filters))(avg_pool)\n",
    "    max_pool = layers.Reshape((1, 1, filters))(max_pool)\n",
    "    avg_pool = layers.Dense(filters//8, activation='relu')(avg_pool)\n",
    "    avg_pool = layers.Dense(filters, activation='sigmoid')(avg_pool)\n",
    "    max_pool = layers.Dense(filters//8, activation='relu')(max_pool)\n",
    "    max_pool = layers.Dense(filters, activation='sigmoid')(max_pool)\n",
    "    attention = layers.Add()([avg_pool, max_pool])\n",
    "    return layers.Multiply()([x, attention])\n",
    "\n",
    "def dual_attention_block(x, filters):\n",
    "    x = spatial_attention_block(x)  # Select rice leaves\n",
    "    x = channel_attention_block(x, filters)  # Focus on diseases\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your original model structure with key improvements\n",
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "# Add data augmentation to your model\n",
    "cnn.add(data_augmentation)\n",
    "cnn.add(layers.Rescaling(1./255))\n",
    "\n",
    "# Block 1 - Your original structure\n",
    "cnn.add(layers.Conv2D(32, 3, padding='same', activation='relu', input_shape=(200, 200, 3)))\n",
    "cnn.add(layers.BatchNormalization())\n",
    "cnn.add(layers.Conv2D(32, 3, activation='relu'))\n",
    "cnn.add(layers.BatchNormalization())\n",
    "cnn.add(layers.MaxPooling2D(2, 2))\n",
    "cnn.add(layers.Dropout(0.1))  # Add dropout after pooling\n",
    "\n",
    "# Block 2 - Your original structure\n",
    "cnn.add(layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "cnn.add(layers.BatchNormalization())\n",
    "cnn.add(layers.Conv2D(64, 3, activation='relu'))\n",
    "cnn.add(layers.BatchNormalization())\n",
    "cnn.add(layers.MaxPooling2D(2, 2))\n",
    "cnn.add(layers.Dropout(0.1))\n",
    "\n",
    "# Block 3 - Your original structure\n",
    "cnn.add(layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "cnn.add(layers.BatchNormalization())\n",
    "cnn.add(layers.Conv2D(128, 3, activation='relu'))\n",
    "cnn.add(layers.BatchNormalization())\n",
    "cnn.add(layers.MaxPooling2D(2, 2))\n",
    "cnn.add(layers.Dropout(0.2))\n",
    "\n",
    "# Block 4 - Your original structure\n",
    "cnn.add(layers.Conv2D(256, 3, padding='same', activation='relu'))\n",
    "cnn.add(layers.BatchNormalization())\n",
    "cnn.add(layers.Conv2D(256, 3, activation='relu'))\n",
    "cnn.add(layers.BatchNormalization())\n",
    "cnn.add(layers.MaxPooling2D(2, 2))\n",
    "cnn.add(layers.Dropout(0.2))\n",
    "\n",
    "# Block 5 - Your original structure\n",
    "cnn.add(layers.Conv2D(512, 3, padding='same', activation='relu'))\n",
    "cnn.add(layers.BatchNormalization())\n",
    "cnn.add(layers.Conv2D(512, 3, activation='relu'))\n",
    "cnn.add(layers.BatchNormalization())\n",
    "cnn.add(layers.MaxPooling2D(2, 2))\n",
    "\n",
    "# Output - Your original structure with improvements\n",
    "cnn.add(layers.Dropout(0.3))  # Increased from 0.25\n",
    "cnn.add(layers.GlobalAveragePooling2D())\n",
    "cnn.add(layers.Dense(512, activation='relu'))\n",
    "cnn.add(layers.Dropout(0.5))  # Increased from 0.4\n",
    "cnn.add(layers.Dense(num_classes, activation='softmax'))\n\n# Add multi-leaf attention to existing blocks\n# Insert attention layers after conv blocks\nprint(\"Adding multi-leaf attention to existing model...\")\n\n# Note: For Sequential models, we need to rebuild with attention\n# The attention functions are already defined above\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training configuration\n",
    "# Cosine decay learning rate (improvement over fixed rate)\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=len(train_ds) * 50,\n",
    "    alpha=1e-5\n",
    ")\n",
    "\n",
    "# Better optimizer (AdamW instead of Adam)\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=lr_schedule,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Your original loss with label smoothing\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
    "\n",
    "cnn.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for balanced training\n",
    "def get_class_weights(dataset):\n",
    "    labels = []\n",
    "    for _, label_batch in dataset:\n",
    "        labels.extend(np.argmax(label_batch.numpy(), axis=1))\n",
    "    \n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(labels),\n",
    "        y=labels\n",
    "    )\n",
    "    return dict(enumerate(class_weights))\n",
    "\n",
    "class_weights = get_class_weights(train_ds)\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,  # Increased patience\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'EnhancedRiceModel.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize dataset performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Train with your original epoch count\n",
    "EPOCHS = 50\n",
    "\n",
    "print(\"Starting enhanced training...\")\n",
    "history = cnn.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "def plot_training_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print best results\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    best_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "    print(f\"\\nBest validation accuracy: {best_val_acc:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and save\n",
    "print(\"Evaluating enhanced model...\")\n",
    "test_loss, test_acc = cnn.evaluate(val_ds, verbose=0)\n",
    "print(f\"Enhanced Model Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Save the enhanced model\n",
    "cnn.save('EnhancedNewestCNN.keras')\n",
    "print(\"Enhanced model saved as 'EnhancedNewestCNN.keras'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
